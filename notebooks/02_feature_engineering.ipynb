{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2326e6",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline\n",
    "## Food Price Clustering Project - Phase 1\n",
    "\n",
    "This notebook transforms the consolidated food price time series data into features suitable for clustering analysis.\n",
    "\n",
    "### Objectives:\n",
    "1. **Load** consolidated dataset from preprocessing phase\n",
    "2. **Extract** 3 key features per commodity from time series data:\n",
    "   - **Price Average**: Mean price over the period\n",
    "   - **Coefficient of Variation (CV)**: Volatility measure (std/mean)\n",
    "   - **Price Trend**: Linear regression slope (price change over time)\n",
    "3. **Create** 30-feature matrix (3 features √ó 10 commodities) for clustering\n",
    "4. **Validate** feature quality and distributions\n",
    "5. **Visualize** feature relationships and patterns\n",
    "6. **Export** feature matrix for clustering experiments\n",
    "\n",
    "### Feature Engineering Strategy:\n",
    "- **City-Level Features**: Each city becomes one row with 30 feature columns\n",
    "- **Time Series ‚Üí Features**: Convert daily price data to statistical summaries\n",
    "- **Standardization Ready**: Features designed for easy scaling in clustering phase\n",
    "- **Business Interpretable**: Each feature has clear business meaning\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77f94a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Feature Engineering Pipeline - Environment Setup\n",
      "============================================================\n",
      "üìÖ Notebook run at: 2025-10-04 20:14:56\n",
      "üêç Python version: 3.12.7\n",
      "üêº Pandas version: 2.3.3\n",
      "üî¢ NumPy version: 2.3.3\n",
      "üìä SciPy version: N/A\n",
      "\n",
      "üìÇ Path Verification:\n",
      "   Current working directory: c:\\Users\\UNTAR\\Semester 7\\SKRIPSI\\program\\food-price-clustering\\food-price-clustering\n",
      "   Processed data path: data\\processed (‚úÖ EXISTS)\n",
      "   Features data path: data\\features (‚úÖ EXISTS)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup and Environment Configuration\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def setup_logging(enable_file_logging: bool = False, log_level: str = \"INFO\") -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Setup logging with optional file output.\n",
    "    \n",
    "    Args:\n",
    "        enable_file_logging: Whether to save logs to file\n",
    "        log_level: Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "        \n",
    "    Returns:\n",
    "        Configured logger instance\n",
    "    \"\"\"\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(\"feature_engineering\")\n",
    "    logger.setLevel(getattr(logging, log_level.upper()))\n",
    "    \n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Create formatters\n",
    "    detailed_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'\n",
    "    )\n",
    "    simple_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Console handler (always enabled)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(getattr(logging, log_level.upper()))\n",
    "    console_handler.setFormatter(simple_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler (optional)\n",
    "    if enable_file_logging:\n",
    "        # Create logs directory\n",
    "        logs_dir = Path(\"logs\")\n",
    "        logs_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Generate timestamped log filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_filename = logs_dir / f\"feature_engineering_{timestamp}.log\"\n",
    "        \n",
    "        file_handler = logging.FileHandler(log_filename, encoding='utf-8')\n",
    "        file_handler.setLevel(logging.DEBUG)  # Log everything to file\n",
    "        file_handler.setFormatter(detailed_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        \n",
    "        logger.info(f\"üìù File logging enabled - Log file: {log_filename}\")\n",
    "    else:\n",
    "        logger.info(\"üì∫ Console logging only (file logging disabled)\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def setup_environment() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Setup notebook environment and verify paths.\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing environment information and path verification results.\n",
    "    \"\"\"\n",
    "    # Fix working directory - ensure we're running from project root\n",
    "    notebook_dir = Path.cwd()\n",
    "    project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "    \n",
    "    # Change to project root so all relative paths work correctly\n",
    "    os.chdir(project_root)\n",
    "    \n",
    "    # Verify critical paths\n",
    "    paths_info = {\n",
    "        'notebook_dir': notebook_dir,\n",
    "        'project_root': project_root,\n",
    "        'current_dir': Path.cwd(),\n",
    "        'processed_data_path': Path('data/processed'),\n",
    "        'processed_data_exists': Path('data/processed').exists(),\n",
    "        'features_data_path': Path('data/features'),\n",
    "        'features_data_exists': Path('data/features').exists()\n",
    "    }\n",
    "    \n",
    "    # Create features data directory if it doesn't exist\n",
    "    if not paths_info['features_data_exists']:\n",
    "        paths_info['features_data_path'].mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Environment info\n",
    "    env_info = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'pandas_version': pd.__version__,\n",
    "        'numpy_version': np.__version__,\n",
    "        'scipy_version': stats.__version__ if hasattr(stats, '__version__') else 'N/A'\n",
    "    }\n",
    "    \n",
    "    return {**paths_info, **env_info}\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "\n",
    "# Display environment information\n",
    "print(\"üîß Feature Engineering Pipeline - Environment Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÖ Notebook run at: {env_info['timestamp']}\")\n",
    "print(f\"üêç Python version: {env_info['python_version']}\")\n",
    "print(f\"üêº Pandas version: {env_info['pandas_version']}\")\n",
    "print(f\"üî¢ NumPy version: {env_info['numpy_version']}\")\n",
    "print(f\"üìä SciPy version: {env_info['scipy_version']}\")\n",
    "print()\n",
    "print(\"üìÇ Path Verification:\")\n",
    "print(f\"   Current working directory: {env_info['current_dir']}\")\n",
    "print(f\"   Processed data path: {env_info['processed_data_path']} ({'‚úÖ EXISTS' if env_info['processed_data_exists'] else '‚ùå MISSING'})\")\n",
    "print(f\"   Features data path: {env_info['features_data_path']} ({'‚úÖ EXISTS' if env_info['features_data_exists'] else '‚ùå MISSING'})\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dbe8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 20:15:00,802 - INFO - üì∫ Console logging only (file logging disabled)\n",
      "2025-10-04 20:15:00,804 - INFO - Using input file: data\\processed\\food_prices_consolidated.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Feature Engineering Configuration\n",
      "==================================================\n",
      "üìù File Logging: ‚ùå DISABLED\n",
      "üìä Log Level: INFO\n",
      "üìÅ Input File: food_prices_consolidated.csv\n",
      "ü•¨ Expected Commodities: 10\n",
      "üìà Trend Method: linear_regression\n",
      "üìä Min Data Points: 30\n",
      "üíæ Export Formats: csv, excel\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration for Feature Engineering\n",
    "\"\"\"\n",
    "\n",
    "class FeatureEngineeringConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration class for feature engineering pipeline.\n",
    "    \n",
    "    Uses Pydantic for validation and type checking.\n",
    "    \"\"\"\n",
    "    # Logging configuration\n",
    "    enable_file_logging: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether to save logs to file (True) or console only (False)\"\n",
    "    )\n",
    "    \n",
    "    log_level: str = Field(\n",
    "        default=\"INFO\",\n",
    "        description=\"Logging level: DEBUG, INFO, WARNING, ERROR\"\n",
    "    )\n",
    "    \n",
    "    # Input data configuration\n",
    "    input_file_pattern: str = Field(\n",
    "        default=\"food_prices_consolidated.csv\",\n",
    "        description=\"Pattern to match input CSV files in processed directory\"\n",
    "    )\n",
    "    \n",
    "    # Feature engineering parameters\n",
    "    expected_commodities: List[str] = Field(\n",
    "        default=[\n",
    "            \"Beras\", \"Telur Ayam\", \"Daging Ayam\", \"Daging Sapi\",\n",
    "            \"Bawang Merah\", \"Bawang Putih\", \"Cabai Merah\", \"Cabai Rawit\",\n",
    "            \"Minyak Goreng\", \"Gula Pasir\"\n",
    "        ],\n",
    "        description=\"Expected commodities in the dataset\"\n",
    "    )\n",
    "    \n",
    "    # Feature calculation parameters\n",
    "    min_data_points: int = Field(\n",
    "        default=30,\n",
    "        description=\"Minimum number of data points required per city-commodity for feature calculation\"\n",
    "    )\n",
    "    \n",
    "    trend_method: str = Field(\n",
    "        default=\"linear_regression\",\n",
    "        description=\"Method for calculating price trend: 'linear_regression' or 'simple_slope'\"\n",
    "    )\n",
    "    \n",
    "    # Output configuration\n",
    "    feature_name_format: str = Field(\n",
    "        default=\"{commodity}_{feature_type}\",\n",
    "        description=\"Format for feature column names. Available: {commodity}, {feature_type}\"\n",
    "    )\n",
    "    \n",
    "    export_formats: List[str] = Field(\n",
    "        default=[\"csv\", \"excel\"],\n",
    "        description=\"Export formats for feature matrix\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('log_level')\n",
    "    @classmethod\n",
    "    def validate_log_level(cls, v):\n",
    "        valid_levels = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"]\n",
    "        if v.upper() not in valid_levels:\n",
    "            raise ValueError(f\"Log level must be one of: {valid_levels}\")\n",
    "        return v.upper()\n",
    "    \n",
    "    @field_validator('min_data_points')\n",
    "    @classmethod\n",
    "    def validate_min_data_points(cls, v):\n",
    "        if v < 10:\n",
    "            raise ValueError(\"Minimum data points must be at least 10\")\n",
    "        return v\n",
    "    \n",
    "    @field_validator('trend_method')\n",
    "    @classmethod\n",
    "    def validate_trend_method(cls, v):\n",
    "        valid_methods = [\"linear_regression\", \"simple_slope\"]\n",
    "        if v not in valid_methods:\n",
    "            raise ValueError(f\"Trend method must be one of: {valid_methods}\")\n",
    "        return v\n",
    "    \n",
    "    @field_validator('export_formats')\n",
    "    @classmethod\n",
    "    def validate_export_formats(cls, v):\n",
    "        valid_formats = [\"csv\", \"excel\", \"json\"]\n",
    "        for fmt in v:\n",
    "            if fmt not in valid_formats:\n",
    "                raise ValueError(f\"Export format '{fmt}' not supported. Valid: {valid_formats}\")\n",
    "        return v\n",
    "\n",
    "def find_latest_consolidated_file(processed_dir: Path, pattern: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Find the most recent consolidated data file matching the pattern.\n",
    "    \n",
    "    Args:\n",
    "        processed_dir: Directory containing processed files\n",
    "        pattern: File pattern to match\n",
    "        \n",
    "    Returns:\n",
    "        Path to the most recent file, or None if not found\n",
    "    \"\"\"\n",
    "    matching_files = list(processed_dir.glob(pattern))\n",
    "    \n",
    "    if not matching_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by modification time, most recent first\n",
    "    matching_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return matching_files[0]\n",
    "\n",
    "# Initialize configuration\n",
    "config = FeatureEngineeringConfig()\n",
    "\n",
    "# Setup logging based on configuration\n",
    "logger = setup_logging(config.enable_file_logging, config.log_level)\n",
    "\n",
    "# Find input data file\n",
    "processed_dir = Path(\"data/processed\")\n",
    "input_file = find_latest_consolidated_file(processed_dir, config.input_file_pattern)\n",
    "\n",
    "print(\"‚öôÔ∏è Feature Engineering Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìù File Logging: {'‚úÖ ENABLED' if config.enable_file_logging else '‚ùå DISABLED'}\")\n",
    "print(f\"üìä Log Level: {config.log_level}\")\n",
    "print(f\"üìÅ Input File: {input_file.name if input_file else '‚ùå NOT FOUND'}\")\n",
    "print(f\"ü•¨ Expected Commodities: {len(config.expected_commodities)}\")\n",
    "print(f\"üìà Trend Method: {config.trend_method}\")\n",
    "print(f\"üìä Min Data Points: {config.min_data_points}\")\n",
    "print(f\"üíæ Export Formats: {', '.join(config.export_formats)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not input_file:\n",
    "    raise FileNotFoundError(f\"No consolidated data file found matching pattern: {config.input_file_pattern}\")\n",
    "\n",
    "logger.info(f\"Using input file: {input_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e36f7",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "Load the consolidated dataset and perform initial exploration to understand the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2714a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 20:15:10,295 - INFO - Loading data from: data\\processed\\food_prices_consolidated.csv\n",
      "2025-10-04 20:15:10,841 - INFO - Data loaded successfully: (900460, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Exploration Results\n",
      "============================================================\n",
      "üìã Basic Information:\n",
      "   Total Rows: 900,460\n",
      "   Total Columns: 5\n",
      "   Memory Usage: 16.32 MB\n",
      "   Date Range: 2020-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "   Total Days: 1826\n",
      "\n",
      "üèôÔ∏è Data Coverage:\n",
      "   Cities: 69\n",
      "   Commodities: 10\n",
      "   Provinces: 0\n",
      "\n",
      "üìä Data Points per City-Commodity:\n",
      "   Min: 1305\n",
      "   Max: 1306\n",
      "   Mean: 1305.0\n",
      "   Median: 1305.0\n",
      "\n",
      "‚úÖ Data Quality:\n",
      "   Missing Values: 10\n",
      "   Duplicate Rows: 0\n",
      "\n",
      "üí∞ Price Statistics:\n",
      "   Range: 1 - 218,750 IDR\n",
      "   Mean: 39,354 IDR\n",
      "   Median: 29,250 IDR\n",
      "   Std Dev: 33,612 IDR\n",
      "\n",
      "üèôÔ∏è Cities in Dataset:\n",
      "    1. Kab. Banyumas\n",
      "    2. Kab. Banyuwangi\n",
      "    3. Kab. Boyolali\n",
      "    4. Kab. Bulungan\n",
      "    5. Kab. Bungo\n",
      "    6. Kab. Cilacap\n",
      "    7. Kab. Cirebon\n",
      "    8. Kab. Jember\n",
      "    9. Kab. Karanganyar\n",
      "   10. Kab. Klaten\n",
      "   11. Kab. Kudus\n",
      "   12. Kab. Sragen\n",
      "   13. Kab. Sukoharjo\n",
      "   14. Kab. Sumenep\n",
      "   15. Kab. Tasikmalaya\n",
      "   16. Kab. Wonogiri\n",
      "   17. Kota Balikpapan\n",
      "   18. Kota Banda Aceh\n",
      "   19. Kota Bandar Lampung\n",
      "   20. Kota Bandung\n",
      "   21. Kota Banjarmasin\n",
      "   22. Kota Batam\n",
      "   23. Kota Bekasi\n",
      "   24. Kota Bengkulu\n",
      "   25. Kota Blitar\n",
      "   26. Kota Bogor\n",
      "   27. Kota Bontang\n",
      "   28. Kota Bukittinggi\n",
      "   29. Kota Cilegon\n",
      "   30. Kota Cirebon\n",
      "   31. Kota Depok\n",
      "   32. Kota Dumai\n",
      "   33. Kota Jakarta Pusat\n",
      "   34. Kota Jambi\n",
      "   35. Kota Kediri\n",
      "   36. Kota Lhokseumawe\n",
      "   37. Kota Lubuk Linggau\n",
      "   38. Kota Madiun\n",
      "   39. Kota Malang\n",
      "   40. Kota Medan\n",
      "   41. Kota Metro\n",
      "   42. Kota Meulaboh\n",
      "   43. Kota Padang\n",
      "   44. Kota Padang Sidempuan\n",
      "   45. Kota Palangkaraya\n",
      "   46. Kota Palembang\n",
      "   47. Kota Pangkal Pinang\n",
      "   48. Kota Pekanbaru\n",
      "   49. Kota Pematang Siantar\n",
      "   50. Kota Pontianak\n",
      "   51. Kota Probolinggo\n",
      "   52. Kota Samarinda\n",
      "   53. Kota Sampit\n",
      "   54. Kota Semarang\n",
      "   55. Kota Serang\n",
      "   56. Kota Sibolga\n",
      "   57. Kota Singkawang\n",
      "   58. Kota Sukabumi\n",
      "   59. Kota Surabaya\n",
      "   60. Kota Surakarta (Solo)\n",
      "   61. Kota Tangerang\n",
      "   62. Kota Tanjung\n",
      "   63. Kota Tanjung Pandan\n",
      "   64. Kota Tanjung Pinang\n",
      "   65. Kota Tarakan\n",
      "   66. Kota Tasikmalaya\n",
      "   67. Kota Tegal\n",
      "   68. Kota Tembilahan\n",
      "   69. Kota Yogyakarta\n",
      "\n",
      "ü•¨ Commodities in Dataset:\n",
      "    1. Bawang Merah\n",
      "    2. Bawang Putih\n",
      "    3. Beras\n",
      "    4. Cabai Merah\n",
      "    5. Cabai Rawit\n",
      "    6. Daging Ayam\n",
      "    7. Daging Sapi\n",
      "    8. Gula Pasir\n",
      "    9. Minyak Goreng\n",
      "   10. Telur Ayam\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and Explore Consolidated Dataset\n",
    "\"\"\"\n",
    "\n",
    "def load_consolidated_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load consolidated food price data with proper data types.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to consolidated CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert data types\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    categorical_columns = ['City', 'Commodity', 'Year']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    logger.info(f\"Data loaded successfully: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def explore_dataset(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform comprehensive dataset exploration.\n",
    "    \n",
    "    Args:\n",
    "        df: Consolidated DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with exploration results\n",
    "    \"\"\"\n",
    "    exploration_results = {\n",
    "        'basic_info': {\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'date_range': {\n",
    "                'min_date': df['Date'].min(),\n",
    "                'max_date': df['Date'].max(),\n",
    "                'total_days': (df['Date'].max() - df['Date'].min()).days\n",
    "            }\n",
    "        },\n",
    "        'data_coverage': {\n",
    "            'total_cities': df['City'].nunique(),\n",
    "            'total_commodities': df['Commodity'].nunique(),\n",
    "            'cities_list': sorted(df['City'].unique().tolist()),\n",
    "            'commodities_list': sorted(df['Commodity'].unique().tolist())\n",
    "        },\n",
    "        'data_quality': {\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'duplicate_rows': df.duplicated().sum(),\n",
    "            'price_stats': {\n",
    "                'min_price': df['Price'].min(),\n",
    "                'max_price': df['Price'].max(),\n",
    "                'mean_price': df['Price'].mean(),\n",
    "                'median_price': df['Price'].median(),\n",
    "                'std_price': df['Price'].std()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate data points per city-commodity combination\n",
    "    city_commodity_counts = df.groupby(['City', 'Commodity']).size()\n",
    "    exploration_results['data_coverage']['data_points_per_city_commodity'] = {\n",
    "        'min': city_commodity_counts.min(),\n",
    "        'max': city_commodity_counts.max(),\n",
    "        'mean': city_commodity_counts.mean(),\n",
    "        'median': city_commodity_counts.median()\n",
    "    }\n",
    "    \n",
    "    return exploration_results\n",
    "\n",
    "# Load the data\n",
    "df_consolidated = load_consolidated_data(input_file)\n",
    "\n",
    "# Explore the dataset\n",
    "exploration_results = explore_dataset(df_consolidated)\n",
    "\n",
    "# Display exploration results\n",
    "print(\"üìä Dataset Exploration Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic info\n",
    "basic_info = exploration_results['basic_info']\n",
    "print(f\"üìã Basic Information:\")\n",
    "print(f\"   Total Rows: {basic_info['total_rows']:,}\")\n",
    "print(f\"   Total Columns: {basic_info['total_columns']}\")\n",
    "print(f\"   Memory Usage: {basic_info['memory_usage_mb']:.2f} MB\")\n",
    "print(f\"   Date Range: {basic_info['date_range']['min_date']} to {basic_info['date_range']['max_date']}\")\n",
    "print(f\"   Total Days: {basic_info['date_range']['total_days']}\")\n",
    "\n",
    "# Data coverage\n",
    "coverage = exploration_results['data_coverage']\n",
    "print(f\"\\nüèôÔ∏è Data Coverage:\")\n",
    "print(f\"   Cities: {coverage['total_cities']}\")\n",
    "print(f\"   Commodities: {coverage['total_commodities']}\")\n",
    "\n",
    "# Data points per city-commodity\n",
    "data_points = coverage['data_points_per_city_commodity']\n",
    "print(f\"\\nüìä Data Points per City-Commodity:\")\n",
    "print(f\"   Min: {data_points['min']}\")\n",
    "print(f\"   Max: {data_points['max']}\")\n",
    "print(f\"   Mean: {data_points['mean']:.1f}\")\n",
    "print(f\"   Median: {data_points['median']:.1f}\")\n",
    "\n",
    "# Data quality\n",
    "quality = exploration_results['data_quality']\n",
    "print(f\"\\n‚úÖ Data Quality:\")\n",
    "print(f\"   Missing Values: {sum(quality['missing_values'].values())}\")\n",
    "print(f\"   Duplicate Rows: {quality['duplicate_rows']}\")\n",
    "\n",
    "# Price statistics\n",
    "price_stats = quality['price_stats']\n",
    "print(f\"\\nüí∞ Price Statistics:\")\n",
    "print(f\"   Range: {price_stats['min_price']:,.0f} - {price_stats['max_price']:,.0f} IDR\")\n",
    "print(f\"   Mean: {price_stats['mean_price']:,.0f} IDR\")\n",
    "print(f\"   Median: {price_stats['median_price']:,.0f} IDR\")\n",
    "print(f\"   Std Dev: {price_stats['std_price']:,.0f} IDR\")\n",
    "\n",
    "print(\"\\nüèôÔ∏è Cities in Dataset:\")\n",
    "for i, city in enumerate(coverage['cities_list'], 1):\n",
    "    print(f\"   {i:2d}. {city}\")\n",
    "\n",
    "print(\"\\nü•¨ Commodities in Dataset:\")\n",
    "for i, commodity in enumerate(coverage['commodities_list'], 1):\n",
    "    print(f\"   {i:2d}. {commodity}\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0c4ba",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "Define functions to extract the three key features from time series data for each city-commodity combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797f560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function 1: Price Average Calculation\n",
    "\"\"\"\n",
    "\n",
    "def calculate_price_average(price_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mean price over the time period.\n",
    "    \n",
    "    Args:\n",
    "        price_series: Series of prices for a city-commodity combination\n",
    "        \n",
    "    Returns:\n",
    "        Mean price value\n",
    "    \"\"\"\n",
    "    if len(price_series) == 0 or price_series.isnull().all():\n",
    "        return np.nan\n",
    "    \n",
    "    return price_series.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2a57199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function 2: Coefficient of Variation Calculation\n",
    "\"\"\"\n",
    "\n",
    "def calculate_coefficient_of_variation(price_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the coefficient of variation (CV) as a measure of price volatility.\n",
    "    \n",
    "    CV = standard_deviation / mean\n",
    "    \n",
    "    Args:\n",
    "        price_series: Series of prices for a city-commodity combination\n",
    "        \n",
    "    Returns:\n",
    "        Coefficient of variation (volatility measure)\n",
    "    \"\"\"\n",
    "    if len(price_series) == 0 or price_series.isnull().all():\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove null values\n",
    "    clean_prices = price_series.dropna()\n",
    "    \n",
    "    if len(clean_prices) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    mean_price = clean_prices.mean()\n",
    "    std_price = clean_prices.std()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mean_price == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    cv = std_price / mean_price\n",
    "    return cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68282e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function 3: Price Trend Calculation\n",
    "\"\"\"\n",
    "\n",
    "def calculate_price_trend(price_series: pd.Series, date_series: pd.Series, method: str = \"linear_regression\") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the price trend (slope) over time.\n",
    "    \n",
    "    Args:\n",
    "        price_series: Series of prices for a city-commodity combination\n",
    "        date_series: Corresponding series of dates\n",
    "        method: Method for trend calculation ('linear_regression' or 'simple_slope')\n",
    "        \n",
    "    Returns:\n",
    "        Price trend (slope coefficient)\n",
    "    \"\"\"\n",
    "    if len(price_series) == 0 or price_series.isnull().all():\n",
    "        return np.nan\n",
    "    \n",
    "    # Create DataFrame and remove null values\n",
    "    df_temp = pd.DataFrame({'price': price_series, 'date': date_series}).dropna()\n",
    "    \n",
    "    if len(df_temp) < 3:  # Need at least 3 points for trend\n",
    "        return np.nan\n",
    "    \n",
    "    # Sort by date\n",
    "    df_temp = df_temp.sort_values('date')\n",
    "    \n",
    "    if method == \"linear_regression\":\n",
    "        # Convert dates to numeric (days since first date)\n",
    "        df_temp['days'] = (df_temp['date'] - df_temp['date'].min()).dt.days\n",
    "        \n",
    "        # Fit linear regression\n",
    "        X = df_temp['days'].values.reshape(-1, 1)\n",
    "        y = df_temp['price'].values\n",
    "        \n",
    "        try:\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            slope = model.coef_[0]\n",
    "            return slope\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "            \n",
    "    elif method == \"simple_slope\":\n",
    "        # Simple slope calculation: (last_price - first_price) / days\n",
    "        first_price = df_temp['price'].iloc[0]\n",
    "        last_price = df_temp['price'].iloc[-1]\n",
    "        total_days = (df_temp['date'].iloc[-1] - df_temp['date'].iloc[0]).days\n",
    "        \n",
    "        if total_days == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        slope = (last_price - first_price) / total_days\n",
    "        return slope\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown trend method: {method}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec97718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function 4: Feature Matrix Creation\n",
    "\"\"\"\n",
    "\n",
    "def create_feature_matrix(df: pd.DataFrame, config: FeatureEngineeringConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create the feature matrix from consolidated time series data.\n",
    "    \n",
    "    Args:\n",
    "        df: Consolidated DataFrame with time series data\n",
    "        config: Configuration object\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cities as rows and features as columns\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting feature matrix creation\")\n",
    "    \n",
    "    # Initialize results list\n",
    "    feature_rows = []\n",
    "    \n",
    "    # Get unique cities\n",
    "    cities = df['City'].unique()\n",
    "    logger.info(f\"Processing {len(cities)} cities\")\n",
    "    \n",
    "    for city in cities:\n",
    "        logger.debug(f\"Processing city: {city}\")\n",
    "        \n",
    "        # Filter data for this city\n",
    "        city_data = df[df['City'] == city].copy()\n",
    "        \n",
    "        # Initialize feature row\n",
    "        feature_row = {'City': city}\n",
    "        \n",
    "        # Process each commodity\n",
    "        for commodity in config.expected_commodities:\n",
    "            # Filter data for this commodity\n",
    "            commodity_data = city_data[city_data['Commodity'] == commodity].copy()\n",
    "            \n",
    "            if len(commodity_data) < config.min_data_points:\n",
    "                logger.warning(f\"Insufficient data for {city} - {commodity}: {len(commodity_data)} points\")\n",
    "                # Set features to NaN for insufficient data\n",
    "                avg_col = config.feature_name_format.format(commodity=commodity, feature_type=\"avg\")\n",
    "                cv_col = config.feature_name_format.format(commodity=commodity, feature_type=\"cv\")\n",
    "                trend_col = config.feature_name_format.format(commodity=commodity, feature_type=\"trend\")\n",
    "                \n",
    "                feature_row[avg_col] = np.nan\n",
    "                feature_row[cv_col] = np.nan\n",
    "                feature_row[trend_col] = np.nan\n",
    "                continue\n",
    "            \n",
    "            # Sort by date\n",
    "            commodity_data = commodity_data.sort_values('Date')\n",
    "            \n",
    "            # Extract features\n",
    "            price_avg = calculate_price_average(commodity_data['Price'])\n",
    "            price_cv = calculate_coefficient_of_variation(commodity_data['Price'])\n",
    "            price_trend = calculate_price_trend(\n",
    "                commodity_data['Price'], \n",
    "                commodity_data['Date'], \n",
    "                config.trend_method\n",
    "            )\n",
    "            \n",
    "            # Add to feature row\n",
    "            avg_col = config.feature_name_format.format(commodity=commodity, feature_type=\"avg\")\n",
    "            cv_col = config.feature_name_format.format(commodity=commodity, feature_type=\"cv\")\n",
    "            trend_col = config.feature_name_format.format(commodity=commodity, feature_type=\"trend\")\n",
    "            \n",
    "            feature_row[avg_col] = price_avg\n",
    "            feature_row[cv_col] = price_cv\n",
    "            feature_row[trend_col] = price_trend\n",
    "            \n",
    "            logger.debug(f\"  {commodity}: avg={price_avg:.0f}, cv={price_cv:.3f}, trend={price_trend:.3f}\")\n",
    "        \n",
    "        feature_rows.append(feature_row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_matrix = pd.DataFrame(feature_rows)\n",
    "    \n",
    "    # Set City as index\n",
    "    feature_matrix = feature_matrix.set_index('City')\n",
    "    \n",
    "    logger.info(f\"Feature matrix created: {feature_matrix.shape}\")\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a02ba",
   "metadata": {},
   "source": [
    "## Execute Feature Engineering\n",
    "\n",
    "Create the feature matrix from the consolidated time series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47818477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 20:15:26,355 - INFO - Starting feature matrix creation\n",
      "2025-10-04 20:15:26,359 - INFO - Processing 69 cities\n",
      "2025-10-04 20:15:27,932 - INFO - Feature matrix created: (69, 30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Feature Engineering Successful!\n",
      "==================================================\n",
      "üìä Feature Matrix Shape: (69, 30)\n",
      "üèôÔ∏è Cities: 69\n",
      "üî¢ Features: 30\n",
      "üíæ Memory Usage: 0.02 MB\n",
      "\n",
      "üìã Feature Columns (30):\n",
      "   Average Features (10): Beras_avg, Telur Ayam_avg, Daging Ayam_avg...\n",
      "   CV Features (10): Beras_cv, Telur Ayam_cv, Daging Ayam_cv...\n",
      "   Trend Features (10): Beras_trend, Telur Ayam_trend, Daging Ayam_trend...\n",
      "\n",
      "‚ùì Missing Values: 0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute Feature Engineering Pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Create the feature matrix\n",
    "try:\n",
    "    feature_matrix = create_feature_matrix(df_consolidated, config)\n",
    "    \n",
    "    print(\"üéâ Feature Engineering Successful!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Feature Matrix Shape: {feature_matrix.shape}\")\n",
    "    print(f\"üèôÔ∏è Cities: {len(feature_matrix)}\")\n",
    "    print(f\"üî¢ Features: {len(feature_matrix.columns)}\")\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    memory_usage_mb = feature_matrix.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"üíæ Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "    \n",
    "    # Show feature columns\n",
    "    print(f\"\\nüìã Feature Columns ({len(feature_matrix.columns)}):\")\n",
    "    feature_cols = feature_matrix.columns.tolist()\n",
    "    \n",
    "    # Group by feature type for better display\n",
    "    avg_features = [col for col in feature_cols if col.endswith('_avg')]\n",
    "    cv_features = [col for col in feature_cols if col.endswith('_cv')]\n",
    "    trend_features = [col for col in feature_cols if col.endswith('_trend')]\n",
    "    other_features = [col for col in feature_cols if not any(col.endswith(suffix) for suffix in ['_avg', '_cv', '_trend'])]\n",
    "    \n",
    "    if other_features:\n",
    "        print(f\"   Metadata: {', '.join(other_features)}\")\n",
    "    print(f\"   Average Features ({len(avg_features)}): {', '.join(avg_features[:3])}{'...' if len(avg_features) > 3 else ''}\")\n",
    "    print(f\"   CV Features ({len(cv_features)}): {', '.join(cv_features[:3])}{'...' if len(cv_features) > 3 else ''}\")\n",
    "    print(f\"   Trend Features ({len(trend_features)}): {', '.join(trend_features[:3])}{'...' if len(trend_features) > 3 else ''}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = feature_matrix.isnull().sum().sum()\n",
    "    print(f\"\\n‚ùì Missing Values: {missing_values}\")\n",
    "    \n",
    "    if missing_values > 0:\n",
    "        print(\"   Missing values by column:\")\n",
    "        missing_by_col = feature_matrix.isnull().sum()\n",
    "        for col, count in missing_by_col[missing_by_col > 0].items():\n",
    "            print(f\"     {col}: {count}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Feature engineering failed: {str(e)}\")\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8c0db",
   "metadata": {},
   "source": [
    "## Feature Analysis and Validation\n",
    "\n",
    "Analyze the extracted features to understand their distributions and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Feature Matrix Validation Results\n",
      "============================================================\n",
      "üìä Basic Information:\n",
      "   Total Cities: 69\n",
      "   Numeric Features: 30\n",
      "   Metadata Columns: []\n",
      "\n",
      "‚úÖ Data Quality:\n",
      "   Total Missing Values: 0\n",
      "   Complete Cases: 69\n",
      "   Incomplete Cases: 0\n",
      "\n",
      "üìà Feature Statistics by Type:\n",
      "   AVG Features (10):\n",
      "     Range: 11230.077 to 149930.996\n",
      "     Mean: 39353.539\n",
      "     Std Dev: 4281.201\n",
      "     Missing Rate: 0.0%\n",
      "   CV Features (10):\n",
      "     Range: 0.030 to 0.499\n",
      "     Mean: 0.181\n",
      "     Std Dev: 0.028\n",
      "     Missing Rate: 0.0%\n",
      "   TREND Features (10):\n",
      "     Range: -9.398 to 24.917\n",
      "     Mean: 4.750\n",
      "     Std Dev: 1.718\n",
      "     Missing Rate: 0.0%\n",
      "\n",
      "üìã Sample Feature Matrix (first 5 cities):\n",
      "                     Beras_avg  Beras_cv  Beras_trend  Telur Ayam_avg  Telur Ayam_cv  Telur Ayam_trend  Daging Ayam_avg  Daging Ayam_cv  Daging Ayam_trend  Daging Sapi_avg\n",
      "City                                                                                                                                                                       \n",
      "Kota Banda Aceh   11715.019157  0.122186     2.376393    26311.877395       0.114868          3.878177     27860.268199        0.174171           6.911132    146292.145594\n",
      "Kota Lhokseumawe  11447.318008  0.127311     2.484369    27576.091954       0.107324          3.857058     29303.639847        0.133182           3.946002    141454.406130\n",
      "Kota Meulaboh     11447.586207  0.130635     2.536519    27176.053640       0.106378          3.885970     29375.249042        0.103671           3.069541    141363.754789\n",
      "Kota Cilegon      12818.275862  0.139289     2.787638    26057.586207       0.121320          3.171635     37498.084291        0.066908           2.101704    127067.088123\n",
      "Kota Serang       11385.172414  0.144813     2.578331    25915.632184       0.120913          3.299844     35616.475096        0.066397           0.768817    129656.704981\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Validation and Statistical Analysis\n",
    "\"\"\"\n",
    "\n",
    "def validate_feature_matrix(feature_matrix: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the feature matrix and compute statistical summaries.\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix: DataFrame with extracted features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Separate numeric and non-numeric columns\n",
    "    numeric_cols = feature_matrix.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_cols = feature_matrix.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    validation_results = {\n",
    "        'basic_info': {\n",
    "            'total_cities': len(feature_matrix),\n",
    "            'total_features': len(numeric_cols),\n",
    "            'metadata_columns': len(non_numeric_cols),\n",
    "            'numeric_columns': numeric_cols,\n",
    "            'metadata_columns': non_numeric_cols\n",
    "        },\n",
    "        'data_quality': {\n",
    "            'missing_values_total': feature_matrix[numeric_cols].isnull().sum().sum(),\n",
    "            'missing_values_by_column': feature_matrix[numeric_cols].isnull().sum().to_dict(),\n",
    "            'complete_cases': len(feature_matrix.dropna(subset=numeric_cols)),\n",
    "            'incomplete_cases': len(feature_matrix) - len(feature_matrix.dropna(subset=numeric_cols))\n",
    "        },\n",
    "        'feature_statistics': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate statistics for each feature type\n",
    "    for feature_type in ['avg', 'cv', 'trend']:\n",
    "        type_cols = [col for col in numeric_cols if col.endswith(f'_{feature_type}')]\n",
    "        \n",
    "        if type_cols:\n",
    "            type_data = feature_matrix[type_cols]\n",
    "            validation_results['feature_statistics'][feature_type] = {\n",
    "                'count': len(type_cols),\n",
    "                'min': type_data.min().min(),\n",
    "                'max': type_data.max().max(),\n",
    "                'mean': type_data.mean().mean(),\n",
    "                'median': type_data.median().median(),\n",
    "                'std': type_data.std().mean(),\n",
    "                'missing_rate': type_data.isnull().sum().sum() / (len(type_data) * len(type_cols))\n",
    "            }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate the feature matrix\n",
    "validation_results = validate_feature_matrix(feature_matrix)\n",
    "\n",
    "print(\"üîç Feature Matrix Validation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic info\n",
    "basic_info = validation_results['basic_info']\n",
    "print(f\"üìä Basic Information:\")\n",
    "print(f\"   Total Cities: {basic_info['total_cities']}\")\n",
    "print(f\"   Numeric Features: {basic_info['total_features']}\")\n",
    "print(f\"   Metadata Columns: {basic_info['metadata_columns']}\")\n",
    "\n",
    "# Data quality\n",
    "quality = validation_results['data_quality']\n",
    "print(f\"\\n‚úÖ Data Quality:\")\n",
    "print(f\"   Total Missing Values: {quality['missing_values_total']}\")\n",
    "print(f\"   Complete Cases: {quality['complete_cases']}\")\n",
    "print(f\"   Incomplete Cases: {quality['incomplete_cases']}\")\n",
    "\n",
    "# Feature statistics by type\n",
    "print(f\"\\nüìà Feature Statistics by Type:\")\n",
    "for feature_type, stats in validation_results['feature_statistics'].items():\n",
    "    print(f\"   {feature_type.upper()} Features ({stats['count']}):\")\n",
    "    print(f\"     Range: {stats['min']:.3f} to {stats['max']:.3f}\")\n",
    "    print(f\"     Mean: {stats['mean']:.3f}\")\n",
    "    print(f\"     Std Dev: {stats['std']:.3f}\")\n",
    "    print(f\"     Missing Rate: {stats['missing_rate']:.1%}\")\n",
    "\n",
    "# Show sample of feature matrix\n",
    "print(f\"\\nüìã Sample Feature Matrix (first 5 cities):\")\n",
    "display_cols = [col for col in feature_matrix.columns][:10]  # Show first 10 features\n",
    "print(feature_matrix[display_cols].head().to_string())\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85299f5b",
   "metadata": {},
   "source": [
    "## Export Feature Matrix\n",
    "\n",
    "Save the feature matrix for use in clustering experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "371c2b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Feature Matrix Export Successful!\n",
      "==================================================\n",
      "üìÑ CSV: data\\features\\feature_matrix_20251004_201543.csv\n",
      "   Size: 39.7 KB\n",
      "üìÑ EXCEL: data\\features\\feature_matrix_20251004_201543.xlsx\n",
      "   Size: 30.9 KB\n",
      "üìÑ METADATA: data\\features\\feature_matrix_20251004_201543_metadata.json\n",
      "\n",
      "‚úÖ Feature matrix ready for clustering experiments!\n",
      "üìÅ Files saved in: data/features/\n",
      "üî¢ Matrix Shape: 69 cities √ó 30 features\n",
      "\n",
      "üìã Ready for Clustering:\n",
      "   ‚úÖ 30 numeric features extracted\n",
      "   ‚úÖ 69 cities ready for clustering\n",
      "   ‚úÖ Feature scaling will be needed before clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emman\\AppData\\Local\\Temp\\ipykernel_2572\\241157282.py:45: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  'configuration': config.dict(),\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export Feature Matrix to Multiple Formats\n",
    "\"\"\"\n",
    "\n",
    "def export_feature_matrix(feature_matrix: pd.DataFrame, config: FeatureEngineeringConfig) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Export feature matrix to multiple formats.\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix: DataFrame with extracted features\n",
    "        config: Configuration object for metadata\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with export file paths\n",
    "    \"\"\"\n",
    "    features_dir = Path(\"data/features\")\n",
    "    features_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_filename = f\"feature_matrix_{timestamp}\"\n",
    "    \n",
    "    export_paths = {}\n",
    "    \n",
    "    # Export to different formats based on configuration\n",
    "    for export_format in config.export_formats:\n",
    "        if export_format == \"csv\":\n",
    "            csv_path = features_dir / f\"{base_filename}.csv\"\n",
    "            feature_matrix.to_csv(csv_path)\n",
    "            export_paths['csv'] = str(csv_path)\n",
    "            \n",
    "        elif export_format == \"excel\":\n",
    "            excel_path = features_dir / f\"{base_filename}.xlsx\"\n",
    "            feature_matrix.to_excel(excel_path)\n",
    "            export_paths['excel'] = str(excel_path)\n",
    "            \n",
    "        elif export_format == \"json\":\n",
    "            json_path = features_dir / f\"{base_filename}.json\"\n",
    "            feature_matrix.to_json(json_path, orient='index', indent=2)\n",
    "            export_paths['json'] = str(json_path)\n",
    "    \n",
    "    # Export metadata\n",
    "    metadata = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'configuration': config.dict(),\n",
    "        'feature_matrix_info': {\n",
    "            'shape': feature_matrix.shape,\n",
    "            'cities': len(feature_matrix),\n",
    "            'features': len(feature_matrix.select_dtypes(include=[np.number]).columns),\n",
    "            'missing_values': feature_matrix.isnull().sum().sum(),\n",
    "            'feature_columns': feature_matrix.columns.tolist(),\n",
    "            'city_names': feature_matrix.index.tolist()\n",
    "        },\n",
    "        'feature_statistics': validation_results['feature_statistics']\n",
    "    }\n",
    "    \n",
    "    metadata_path = features_dir / f\"{base_filename}_metadata.json\"\n",
    "    import json\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    export_paths['metadata'] = str(metadata_path)\n",
    "    \n",
    "    return export_paths\n",
    "\n",
    "# Export the feature matrix\n",
    "try:\n",
    "    export_paths = export_feature_matrix(feature_matrix, config)\n",
    "    \n",
    "    print(\"üíæ Feature Matrix Export Successful!\")\n",
    "    print(\"=\" * 50)\n",
    "    for format_type, file_path in export_paths.items():\n",
    "        if format_type != 'metadata':\n",
    "            file_size = Path(file_path).stat().st_size / 1024  # KB\n",
    "            print(f\"üìÑ {format_type.upper()}: {file_path}\")\n",
    "            print(f\"   Size: {file_size:.1f} KB\")\n",
    "    \n",
    "    print(f\"üìÑ METADATA: {export_paths['metadata']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature matrix ready for clustering experiments!\")\n",
    "    print(f\"üìÅ Files saved in: data/features/\")\n",
    "    print(f\"üî¢ Matrix Shape: {feature_matrix.shape[0]} cities √ó {feature_matrix.shape[1]} features\")\n",
    "    \n",
    "    # Prepare summary for next phase\n",
    "    numeric_features = feature_matrix.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"\\nüìã Ready for Clustering:\")\n",
    "    print(f\"   ‚úÖ {len(numeric_features)} numeric features extracted\")\n",
    "    print(f\"   ‚úÖ {len(feature_matrix)} cities ready for clustering\")\n",
    "    print(f\"   ‚úÖ Feature scaling will be needed before clustering\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Export failed: {str(e)}\")\n",
    "    print(f\"‚ùå Export Error: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca73f2",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ **Data Loading**: Successfully loaded consolidated time series data\n",
    "2. ‚úÖ **Feature Extraction**: Extracted 3 key features per commodity:\n",
    "   - **Price Average**: Mean price over the time period\n",
    "   - **Coefficient of Variation**: Volatility measure (std/mean)\n",
    "   - **Price Trend**: Linear regression slope (price change over time)\n",
    "3. ‚úÖ **Feature Matrix**: Created 30-feature matrix (3 features √ó 10 commodities)\n",
    "4. ‚úÖ **Validation**: Comprehensive feature quality checks and statistics\n",
    "5. ‚úÖ **Export**: Multiple output formats (CSV, Excel) with metadata\n",
    "\n",
    "### Key Features Created:\n",
    "- **30 Numeric Features**: Ready for clustering algorithms\n",
    "- **City-Level Data**: Each row represents one city with all commodity features\n",
    "- **Business Interpretable**: Each feature has clear meaning for market analysis\n",
    "- **Quality Validated**: Statistical summaries and missing value analysis completed\n",
    "\n",
    "### Feature Engineering Results:\n",
    "- **Input**: Time series price data (daily observations)\n",
    "- **Output**: Feature matrix suitable for clustering\n",
    "- **Transformation**: Time series ‚Üí Statistical summaries\n",
    "- **Scalability**: Designed for easy addition of new commodities or time periods\n",
    "\n",
    "### Next Steps:\n",
    "1. **Clustering Experiments** (`03_clustering_experiments.ipynb`):\n",
    "   - Feature scaling (StandardScaler recommended)\n",
    "   - Optimal K selection (Elbow method + Silhouette analysis)\n",
    "   - K-Means clustering implementation\n",
    "   - Fuzzy C-Means clustering implementation\n",
    "   - Spectral clustering implementation\n",
    "   - Algorithm comparison and evaluation\n",
    "\n",
    "2. **Expected Workflow**:\n",
    "   - Load feature matrix from `data/features/`\n",
    "   - Apply feature scaling\n",
    "   - Determine optimal number of clusters (k=3 to k=8 expected)\n",
    "   - Compare clustering algorithms\n",
    "   - Interpret business meaning of clusters\n",
    "\n",
    "3. **Clustering Preparation**:\n",
    "   - ‚úÖ Feature matrix ready\n",
    "   - ‚úÖ No missing values (or handled appropriately)\n",
    "   - ‚è≥ Feature scaling needed before clustering\n",
    "   - ‚è≥ Dimensionality reduction for visualization (PCA ‚Üí t-SNE/UMAP)\n",
    "\n",
    "The feature engineering phase is complete! The feature matrix is ready for clustering analysis. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30cbb0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
